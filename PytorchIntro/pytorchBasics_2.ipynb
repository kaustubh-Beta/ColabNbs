{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorchBasics_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaustubh-Beta/ColabNbs/blob/master/PytorchIntro/pytorchBasics_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU5bhrfkrUUm",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch basics 2\n",
        "\n",
        "In this tutorial we will learn explore the `autograd` package of pytorch. The autograd package provides automatic differentiation for all operations on the tensors. \n",
        "\n",
        "We will try to understand this package in a better way. \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKd153JDrRgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWxQOXF8GzdM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "5850924d-7704-4aa2-9b20-67ecfb6e1f34"
      },
      "source": [
        "# Creating a tensor \n",
        "x1 = torch.ones(2,2, requires_grad=True)\n",
        "x2 = torch.ones(2,2, requires_grad=False)\n",
        "\n",
        "# NOTE : By setting therequires_grad argument we enable the tracking of computation for the tensor \n",
        "\n",
        "# To understand the working in a better way we perform some operations on the tensor x\n",
        "y1 = x1 + 2\n",
        "y2 = x2 + 2\n",
        "\n",
        "print(\"printing the results for tensor with requires_grad flag is set to true ......\")\n",
        "print(\"Printing the output for the computation : \",y1)\n",
        "print(\"Printing the grad_fn attribute of the computation \\n\")\n",
        "print(y1.grad_fn)\n",
        "\n",
        "print(\"\\n\\nprinting the results for tensor with requires_grad flag is set to false ......\")\n",
        "print(\"Printing the output for the computation : \",y2)\n",
        "print(\"Printing the grad_fn attribute of the computation \\n\")\n",
        "print(y2.grad_fn)\n",
        "\n",
        "# Take a minute and read below text to understand what just happened !! :-O"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing the results for tensor with requires_grad flag is set to true ......\n",
            "Printing the output for the computation :  tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n",
            "Printing the grad_fn attribute of the computation \n",
            "\n",
            "<AddBackward0 object at 0x7fd4761fd550>\n",
            "\n",
            "\n",
            "printing the results for tensor with requires_grad flag is set to false ......\n",
            "Printing the output for the computation :  tensor([[3., 3.],\n",
            "        [3., 3.]])\n",
            "Printing the grad_fn attribute of the computation \n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skFssEvYIZcB",
        "colab_type": "text"
      },
      "source": [
        "### What just happened ??????\n",
        "Once we set the flag `requires_grad = true` for a tensor x1 then any new tensor created because of some operation on x1 will have a `.grad_fn` attribute which would refer a function / operation that was performed on  x1.\n",
        "\n",
        "For tensor with the `required_grad = False` and for tensor created by user, the value of `.grad_fn` = None  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQQbOZIcL43q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7cafdb1a-b0f2-4f55-b89a-857e82d42980"
      },
      "source": [
        "z = y1*y1*3\n",
        "out = z.mean()\n",
        "\n",
        "print(\"Value of z : %r\\n\"%z)\n",
        "print(\"value of out : %r\\n\"%out)\n",
        "\n",
        "# Uncomment the bellow block and read what error is given to understand \n",
        "# that we canonly set the value to True for requires_grad_(). Which means \n",
        "# you can start to log all the function affecting a tensor in middle of the code\n",
        "# but to undo that effect later in the code you cannot write requires_grad_(False)\n",
        "\"\"\"\n",
        "z.requires_grad_(False)\n",
        "print(\"Updated value of z : %r\\n\"%z)\n",
        "\"\"\"\n",
        "\n",
        "# To avoid creating a log for a perticular operation we can simply use below code\n",
        "with torch.no_grad():\n",
        "  z2 = y1*y1*3\n",
        "  out2 = z2.mean()\n",
        "print(out2.grad_fn)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value of z : tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>)\n",
            "\n",
            "value of out : tensor(27., grad_fn=<MeanBackward0>)\n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrtGmrZ6Tjvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2b3ca4ea-b181-4e95-9a7d-b6c490ba22ce"
      },
      "source": [
        "# Performing backpropagation \n",
        "out.backward()\n",
        "print(x1.grad)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbUgF09oUQW_",
        "colab_type": "text"
      },
      "source": [
        "When we set the `.requires_grad`attribute as true for a tensor then all the operations on it are tracked and we can finally call `.backward()` we have all the gradients computed automatically and are stored in `.grad` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdNBnkHd0oqt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "162605f8-f686-4f25-bac6-27505092d56a"
      },
      "source": [
        "a = torch.ones(1, requires_grad = True)\n",
        "b = a*2\n",
        "b = b*2\n",
        "print(b)\n",
        "\n",
        "b.backward()\n",
        "print(a.grad)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4.], grad_fn=<MulBackward0>)\n",
            "tensor([4.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szR_ERcIg4U2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a67388b5-f662-45e4-a237-291d5c02d96f"
      },
      "source": [
        "# when the function y is not a scalar value \n",
        "x = torch.ones(3, requires_grad = True)\n",
        "\n",
        "y = x*2\n",
        "\n",
        "y = y*2\n",
        "\n",
        "print(y)\n",
        "# Note : the autograd is not able to compute the full jacobian directly\n",
        "# But we can calculate vector-jacobian product"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4., 4., 4.], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB4xfo8Gh4ts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "169b3aa7-ea62-46d5-ae56-32d5a941c5ac"
      },
      "source": [
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
        "y.backward(v)\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4.0000e-01, 4.0000e+00, 4.0000e-04])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}